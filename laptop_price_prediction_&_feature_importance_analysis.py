# -*- coding: utf-8 -*-
"""Laptop Price Prediction & Feature Importance Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BQpvx8MJlzjR1qR923DHAg06C0x2Xf_u
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# sklearn preproccesing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer

# regrassion
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.ensemble import VotingRegressor, StackingRegressor

# metrics
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, accuracy_score

from warnings import filterwarnings
filterwarnings('ignore')

df=pd.read_csv('data.csv') #https://www.kaggle.com/datasets/jacksondivakarr/laptop-price-prediction-dataset
df.sample(5)

"""#Y_data profiling

"""

# !pip install ydata-profiling

# from ydata_profiling import ProfileReport

# profile = ProfileReport(df, title="Laptop Price Prediction Report",explorative=True)

# profile.to_file("laptop_price_prediction_report.html")

"""#Preproccesing"""

# Converting resolution to one column
df["screen_pixels"] = df["resolution_width"] * df["resolution_height"]

# droping all unneccesary columns
df.drop(columns=["resolution_width","processor", "resolution_height","Unnamed: 0.1","Unnamed: 0","name"],inplace=True)

# converting pricing to taka (it was on rupee before)
df["price_tk"] = df["price"] * 1.36
df.drop(columns=["price"],inplace=True)

df.sample(5)

# creating gpu type

def get_gpu_type(gpu):
    gpu = str(gpu).lower()
    if any(x in gpu for x in ["nvidia", "rtx", "gtx", "geforce","Radeon Pro"]):
        return "Dedicated"
    elif any(x in gpu for x in ["intel", "iris", "uhd", "integrated", "apple", "amd", "arm", "core"]):
        return "Integrated"
    else:
        return "Unknown"

df["gpu_type"] = df["GPU"].apply(get_gpu_type)

# creating gpu brand
def get_gpu_brand(gpu):
    gpu = str(gpu).lower()
    if "nvidia" in gpu or "rtx" in gpu or "gtx" in gpu:
        return "NVIDIA"
    elif "amd" in gpu or "radeon" in gpu:
        return "AMD"
    elif "intel" in gpu:
        return "Intel"
    elif "apple" in gpu:
        return "Apple"
    else:
        return "Other"

df["gpu_brand"] = df["GPU"].apply(get_gpu_brand)

# creating gpu tier
import re

def get_gpu_tier(gpu):
    gpu = str(gpu).lower()
    gpu = re.sub(r"\s+", " ", gpu)

    # Apple GPUs (integrated but high-performing)
    if "apple" in gpu or "m1" in gpu or "m2" in gpu:
        return "Mid"

    # High-end NVIDIA RTX
    if re.search(r"rtx\s*(40[7-9]0)", gpu):
        return "High"

    # Mid-range NVIDIA RTX
    if re.search(r"rtx\s*(30[5-8]0|40[5-6]0|20[5-6]0)", gpu):
        return "Mid"

    # Low-end NVIDIA
    if "gtx" in gpu or "mx" in gpu:
        return "Low"

    # AMD high-end
    if re.search(r"rx\s*(78|79)\d{2}", gpu):
        return "High"

    # AMD mid-range
    if re.search(r"rx\s*(55|56|66|67)\d{2}", gpu):
        return "Mid"

    # AMD integrated
    if "vega" in gpu:
        return "Low"

    # Intel integrated
    if any(x in gpu for x in ["iris", "uhd", "intel"]):
        return "Low"

    return "Unknown"



df["gpu_tier"] = df["GPU"].apply(get_gpu_tier)
df.loc[
    (df["gpu_tier"] == "Unknown") & (df["gpu_type"] == "Integrated"),
    "gpu_tier"
] = "Low"

# droping rare cases
df.drop(index=215, inplace=True)
df.drop(index=565, inplace=True)

df['gpu_tier'].value_counts()

def extract_cpu_cores(cpu):
    if pd.isna(cpu):
        return np.nan

    cpu = cpu.lower()

    # Numeric cores (e.g., "6 cores")
    match = re.search(r'(\d+)\s*core', cpu)
    if match:
        return int(match.group(1))

    # Word-based cores
    core_map = {
        'dual': 2,
        'quad': 4,
        'hexa': 6,
        'octa': 8,
        'deca': 10
    }

    for word, value in core_map.items():
        if word in cpu:
            return value

    return np.nan

def extract_cpu_threads(cpu):
    if pd.isna(cpu):
        return np.nan

    cpu = cpu.lower()

    # Numeric threads (e.g., "12 threads")
    match = re.search(r'(\d+)\s*thread', cpu)
    if match:
        return int(match.group(1))

    return np.nan

df['cpu_cores'] = df['CPU'].apply(extract_cpu_cores)
df['cpu_threads'] = df['CPU'].apply(extract_cpu_threads)

df.sample(5)

df['cpu_threads'].value_counts()

print(df['cpu_threads'].median())

df['cpu_cores'].fillna(df['cpu_cores'].median(), inplace=True)
df['cpu_threads'].fillna(df['cpu_threads'].median(), inplace=True)

sns.displot(df['cpu_cores'])

df['ROM_type'].value_counts()

df["is_ssd"] = (df["ROM_type"] == "SSD").astype(int)

df["is_integrated"] = (df["gpu_type"] == "Integrated").astype(int)

sns.displot(df['display_size'])

df['Ram_type'].value_counts()

df.drop(columns=['CPU','GPU','ROM_type','gpu_type'],inplace=True)

df.sample(5)

"""#pipelines

"""

X=df.drop(columns=['price_tk'])
Y=df['price_tk']

numeric_features = X.select_dtypes(include = ['int64','float64']).columns
categorical_features = X.select_dtypes(include = ['object']).columns

# for num
num_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# for cat
cat_transformer = Pipeline(
    steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ]
)

#combine them
preprocessor = ColumnTransformer(
    transformers= [
        ('num',num_transformer,numeric_features),
        ('cat',cat_transformer,categorical_features)
    ]
    )

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# base learner
lr = LinearRegression()
rf = RandomForestRegressor()
gb = GradientBoostingRegressor()

#voting regressor
voting_reg = VotingRegressor(
estimators=[("lr",lr),
            ("rf",rf),
            ("gb",gb)])

#stacking regressor
stack_reg = StackingRegressor(estimators=[("rf",rf),("gb",gb)],final_estimator=Ridge())

"""# training model"""

# dictionary of all models
model_to_run = {
    "Linear Regression": lr,
    "Random Forest": rf,
    "Gradient Boosting": gb,
    "Voting Regressor": voting_reg,
    "Stacking Regressor": stack_reg
}

"""# training & evaluation"""

res = []

for name,model in model_to_run.items():
  pipe = Pipeline(steps=[
      ('preprocesor',preprocessor),
      ('model',model)
  ])

  # train
  pipe.fit(x_train,y_train)

  # predict
  y_pred = pipe.predict(x_test)

  # evaluation
  r2 = r2_score(y_test,y_pred)
  mae =mean_absolute_error(y_test,y_pred)
  mse = mean_squared_error(y_test,y_pred)
  rmse = np.sqrt(mse)


  res.append({
      'model':name,
      'r2':r2,
      'mae':mae,
      'mse':mse,
      'rmse':rmse
  })

rdf = pd.DataFrame(res).sort_values(by='r2',ascending=False)
rdf

"""# visuualisation"""

best_model = rdf.iloc[0]['model']
best_model_obj = model_to_run[best_model]

# final fitting
final_pipe = Pipeline(steps=[
    ('preprocesor',preprocessor),
    ('model',best_model_obj)
])

final_pipe.fit(x_train,y_train)
final_y_pred = final_pipe.predict(x_test)


# plot actual vs best pred
plt.figure(figsize=(10, 6))
plt.scatter(y_test, final_y_pred, alpha=0.5)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red')
plt.xlabel('Actual Price')
plt.ylabel('Predicted Price')
plt.title('Actual vs Predicted Prices')
plt.show()

"""# cross validation"""

from sklearn.model_selection import cross_val_score

rf_pipe = Pipeline(steps=[
    ('preprocessor',preprocessor),
    ('model',RandomForestRegressor(n_estimators=100,random_state=42))
])

# 5 fold cross validation
cv_scores = cross_val_score(rf_pipe,x_train,y_train,cv=5,scoring='neg_mean_squared_error')
cv_rmse = np.sqrt(-cv_scores)
print(cv_rmse)

print(cv_rmse.mean())
print(cv_rmse.std())

"""stacking ensemble"""

stack_pipe = Pipeline(steps=[
    ('preprocessor',preprocessor),
    ('model',stack_reg)
])

# 5 fold cross validation
cv_scores = cross_val_score(stack_pipe,x_train,y_train,cv=5,scoring='neg_mean_squared_error',n_jobs=-1)
cv_rmse = np.sqrt(-cv_scores)
print(cv_rmse)

print(cv_rmse.mean())
print(cv_rmse.std())

"""# greed serach cross validation"""

rf_pipe = Pipeline(steps=[
    ('preprocessor',preprocessor),
    ('model',RandomForestRegressor(n_estimators=100,random_state=42))
])

param_grid = {
    'model__n_estimators': [100, 200],
    'model__max_depth': [None, 10, 20],
    'model__min_samples_split': [2, 5]
}

from sklearn.model_selection import GridSearchCV

greed_scorers = GridSearchCV(
    estimator = rf_pipe,
    param_grid = param_grid,
    scoring = 'neg_mean_squared_error',
    cv = 5,
    verbose = 2,
    n_jobs = -1
)

grid_result = greed_scorers.fit(x_train,y_train)

print(-grid_result.best_score_)
print(grid_result.best_params_)

"""#randomize search cv"""

from scipy.stats import randint

my_dist = randint(1,10)
print(my_dist.rvs(5))

param_dist = {
    'model__n_estimators': randint(100,500),
    'model__max_depth': (None,1,10),
    'model__min_samples_split': randint(2,10)
}

from sklearn.model_selection import RandomizedSearchCV

rand_scorers = RandomizedSearchCV(
    estimator = rf_pipe,
    param_distributions = param_dist,
    n_iter = 50,
    scoring = 'neg_mean_squared_error',
    cv = 5,
    verbose = 2,
    n_jobs = -1,
    random_state = 42
)
rand_scorers.fit(x_train,y_train)

print(-rand_scorers.best_score_)
print(rand_scorers.best_params_)

"""#saving the model with pickling"""

import pickle
filename = 'random_forest_model.plk'

with open(filename, 'wb') as file:
  pickle.dump(rand_scorers, file)

with open('/content/random_forest_model.plk','rb') as file:
  rf_loaded_model=pickle.load(file)

#rf_loaded_model.predict(x_test)
print("done!!!!!")
